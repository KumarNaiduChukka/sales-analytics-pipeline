{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Analytics Pipeline\n",
    "\n",
    "This notebook contains a complete sales analytics pipeline using the Superstore dataset. It includes data ingestion, quality checks, exploratory data analysis, anomaly detection, summary tables, and visualizations."
   ]
  },
  { 
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Import required libraries and set configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set(style='whitegrid')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Define paths\n",
    "RAW_DATA_PATH = '../data/raw/Sample - Superstore.csv'\n",
    "PROCESSED_DATA_PATH = '../data/processed/'\n",
    "FIGURES_PATH = '../outputs/figures/'\n",
    "TABLES_PATH = '../outputs/tables/'\n",
    "REPORTS_PATH = '../reports/'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for path in [PROCESSED_DATA_PATH, FIGURES_PATH, TABLES_PATH, REPORTS_PATH]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Set anomaly detection parameters\n",
    "ZSCORE_THRESHOLD = 3.0  # Z-score threshold for anomaly detection\n",
    "MEDIAN_DEVIATION_THRESHOLD = 0.5  # 50% deviation from median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion\n",
    "\n",
    "Load the raw data from the CSV file and perform initial inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data\n",
    "df_raw = pd.read_csv(RAW_DATA_PATH)\n",
    "\n",
    "# Display basic information\n",
    "print(f'Dataset shape: {df_raw.shape}')\n",
    "print('\nFirst 5 rows:')\n",
    "display(df_raw.head())\n",
    "\n",
    "# Display column information\n",
    "print('\nColumn information:')\n",
    "display(df_raw.info())\n",
    "\n",
    "# Display summary statistics\n",
    "print('\nSummary statistics:')\n",
    "display(df_raw.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Column Mapping\n",
    "\n",
    "Map the original column names to the standardized column names as specified in the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column mapping\n",
    "column_mapping = {\n",
    "    'Order Date': 'Date',\n",
    "    'Order ID': 'OrderID',\n",
    "    'Customer ID': 'CustomerID',\n",
    "    'Region': 'Region',\n",
    "    'Country': 'Country',\n",
    "    'State': 'State',\n",
    "    'Product Name': 'Product',\n",
    "    'Category': 'Category',\n",
    "    'Quantity': 'Quantity',\n",
    "    'Sales': 'Revenue',\n",
    "    'Discount': 'Discount',\n",
    "    'Profit': 'Profit'\n",
    "}\n",
    "\n",
    "# Create a copy of the raw data for processing\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Rename columns based on mapping\n",
    "df = df.rename(columns=column_mapping)\n",
    "\n",
    "# Display the renamed columns\n",
    "print('Renamed columns:')\n",
    "display(df.columns.tolist())\n",
    "\n",
    "# Display the first few rows of the renamed dataframe\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Checks\n",
    "\n",
    "Perform data quality checks to identify issues such as missing values, duplicates, and invalid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "print('Missing values:')\n",
    "display(missing_df[missing_df['Missing Values'] > 0])\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "duplicate_order_ids = df.duplicated('OrderID', keep=False).sum()\n",
    "\n",
    "print(f'\nDuplicate rows: {duplicate_rows}')\n",
    "print(f'Rows with duplicate Order IDs: {duplicate_order_ids}')\n",
    "\n",
    "# Check for invalid dates\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "invalid_dates = df['Date'].isnull().sum()\n",
    "\n",
    "print(f'\nInvalid dates: {invalid_dates}')\n",
    "\n",
    "# Check for negative or zero quantities or revenue\n",
    "negative_zero_quantity = (df['Quantity'] <= 0).sum()\n",
    "negative_zero_revenue = (df['Revenue'] <= 0).sum()\n",
    "\n",
    "print(f'\nNegative or zero quantities: {negative_zero_quantity}')\n",
    "print(f'Negative or zero revenue: {negative_zero_revenue}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data quality report\n",
    "data_quality_report = {\n",
    "    'row_count': len(df),\n",
    "    'column_count': len(df.columns),\n",
    "    'missing_values': {col: int(count) for col, count in missing_values.items() if count > 0},\n",
    "    'duplicate_rows': int(duplicate_rows),\n",
    "    'duplicate_order_ids': int(duplicate_order_ids),\n",
    "    'invalid_dates': int(invalid_dates),\n",
    "    'negative_zero_quantity': int(negative_zero_quantity),\n",
    "    'negative_zero_revenue': int(negative_zero_revenue),\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "# Save data quality report as JSON\n",
    "with open(os.path.join(REPORTS_PATH, 'data_quality_report.json'), 'w') as f:\n",
    "    json.dump(data_quality_report, f, indent=4)\n",
    "\n",
    "print('Data quality report saved to:', os.path.join(REPORTS_PATH, 'data_quality_report.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing and Feature Engineering\n",
    "\n",
    "Clean the data and create new features for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For this dataset, we'll drop rows with missing dates as they are critical\n",
    "# For other missing values, we'll use appropriate strategies\n",
    "\n",
    "df = df.dropna(subset=['Date'])\n",
    "\n",
    "# Fill missing numeric values with appropriate strategies\n",
    "# For quantity, use median\n",
    "if 'Quantity' in missing_df.index and missing_df.loc['Quantity', 'Missing Values'] > 0:\n",
    "    df['Quantity'] = df['Quantity'].fillna(df['Quantity'].median())\n",
    "\n",
    "# For revenue, calculate from quantity if available\n",
    "if 'Revenue' in missing_df.index and missing_df.loc['Revenue', 'Missing Values'] > 0:\n",
    "    # If we have unit price, we can calculate revenue\n",
    "    if 'UnitPrice' in df.columns:\n",
    "        mask = df['Revenue'].isna()\n",
    "        df.loc[mask, 'Revenue'] = df.loc[mask, 'Quantity'] * df.loc[mask, 'UnitPrice']\n",
    "    else:\n",
    "        # Otherwise use median revenue per quantity as an approximation\n",
    "        median_revenue_per_quantity = df[df['Revenue'] > 0]['Revenue'] / df[df['Revenue'] > 0]['Quantity']\n",
    "        median_value = median_revenue_per_quantity.median()\n",
    "        mask = df['Revenue'].isna()\n",
    "        df.loc[mask, 'Revenue'] = df.loc[mask, 'Quantity'] * median_value\n",
    "\n",
    "# Fill remaining missing values with appropriate strategies\n",
    "for col in df.columns:\n",
    "    if df[col].isna().sum() > 0:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Standardize categorical values\n",
    "for col in ['Region', 'Country', 'State', 'Category', 'Product']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].str.strip().str.title()\n",
    "\n",
    "# Create date-based features\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['YearMonth'] = df['Date'].dt.strftime('%Y-%m')\n",
    "df['Quarter'] = df['Date'].dt.quarter\n",
    "df['MonthStart'] = df['Date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Display the preprocessed data\n",
    "print('Preprocessed data:')\n",
    "display(df.head())\n",
    "\n",
    "# Save the cleaned data\n",
    "df.to_csv(os.path.join(PROCESSED_DATA_PATH, 'FactSales_clean.csv'), index=False)\n",
    "print('Cleaned data saved to:', os.path.join(PROCESSED_DATA_PATH, 'FactSales_clean.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis\n",
    "\n",
    "Perform exploratory data analysis to understand the data and identify patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "total_revenue = df['Revenue'].sum()\n",
    "total_orders = df['OrderID'].nunique()\n",
    "avg_order_value = total_revenue / total_orders\n",
    "\n",
    "print(f'Total Revenue: ${total_revenue:,.2f}')\n",
    "print(f'Total Orders: {total_orders:,}')\n",
    "print(f'Average Order Value: ${avg_order_value:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Revenue Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly revenue trend\n",
    "monthly_revenue = df.groupby('YearMonth')['Revenue'].sum().reset_index()\n",
    "monthly_revenue['YearMonth'] = pd.to_datetime(monthly_revenue['YearMonth'] + '-01')\n",
    "monthly_revenue = monthly_revenue.sort_values('YearMonth')\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(monthly_revenue['YearMonth'], monthly_revenue['Revenue'], marker='o', linestyle='-')\n",
    "plt.title('Monthly Revenue Trend', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Revenue ($)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(FIGURES_PATH, 'monthly_revenue_trend.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quarterly revenue trend\n",
    "quarterly_revenue = df.groupby(['Year', 'Quarter'])['Revenue'].sum().reset_index()\n",
    "quarterly_revenue['YearQuarter'] = quarterly_revenue['Year'].astype(str) + '-Q' + quarterly_revenue['Quarter'].astype(str)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(quarterly_revenue['YearQuarter'], quarterly_revenue['Revenue'])\n",
    "plt.title('Quarterly Revenue', fontsize=16)\n",
    "plt.xlabel('Year-Quarter', fontsize=12)\n",
    "plt.ylabel('Revenue ($)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(FIGURES_PATH, 'quarterly_revenue.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Regional Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue by region\n",
    "region_revenue = df.groupby('Region')['Revenue'].sum().reset_index().sort_values('Revenue', ascending=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Region', y='Revenue', data=region_revenue, palette='viridis')\n",
    "plt.title('Revenue by Region', fontsize=16)\n",
    "plt.xlabel('Region', fontsize=12)\n",
    "plt.ylabel('Revenue ($)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(FIGURES_PATH, 'revenue_by_region.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue by state (top 15)\n",
    "state_revenue = df.groupby('State')['Revenue'].sum().reset_index().sort_values('Revenue', ascending=False).head(15)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(x='Revenue', y='State', data=state_revenue, palette='viridis')\n",
    "plt.title('Revenue by State (Top 15)', fontsize=16)\n",
    "plt.xlabel('Revenue ($)', fontsize=12)\n",
    "plt.ylabel('State', fontsize=12)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(FIGURES_PATH, 'revenue_by_state_top15.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Product Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue by category\n",
    "category_revenue = df.groupby('Category')['Revenue'].sum().reset_index().sort_values('Revenue', ascending=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Category', y='Revenue', data=category_revenue, palette='viridis')\n",
    "plt.title('Revenue by Category', fontsize=16)\n",
    "plt.xlabel('Category', fontsize=12)\n",
    "plt.ylabel('Revenue ($)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(FIGURES_PATH, 'revenue_by_category.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 products by revenue\n",
    "top_products = df.groupby('Product')['Revenue'].sum().reset_index().sort_values('Revenue', ascending=False).head(10)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(x='Revenue', y='Product', data=top_products, palette='viridis')\n",
    "plt.title('Top 10 Products by Revenue', fontsize=16)\n",
    "plt.xlabel('Revenue ($)', fontsize=12)\n",
    "plt.ylabel('Product', fontsize=12)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(FIGURES_PATH, 'top10_products_by_revenue.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Customer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 customers by revenue\n",
    "top_customers = df.groupby('CustomerID')['Revenue'].sum().reset_index().sort_values('Revenue', ascending=False).head(10)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(x='Revenue', y='CustomerID', data=top_customers, palette='viridis')\n",
    "plt.title('Top 10 Customers by Revenue', fontsize=16)\n",
    "plt.xlabel('Revenue ($)', fontsize=12)\n",
    "plt.ylabel('Customer ID', fontsize=12)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(FIGURES_PATH, 'top10_customers_by_revenue.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Anomaly Detection\n",
    "\n",
    "Implement anomaly detection methods to identify outliers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Z-Score Method for Order Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total revenue per order\n",
    "order_revenue = df.groupby('OrderID')['Revenue'].sum().reset_index()\n",
    "\n",
    "# Calculate z-scores\n",
    "order_revenue['Revenue_ZScore'] = stats.zscore(order_revenue['Revenue'])\n",
    "\n",
    "# Identify anomalies based on z-score threshold\n",
    "order_revenue['Is_Anomaly'] = abs(order_revenue['Revenue_ZScore']) > ZSCORE_THRESHOLD\n",
    "\n",
    "# Filter anomalies\n",
    "anomalies_zscore = order_revenue[order_revenue['Is_Anomaly']].sort_values('Revenue_ZScore', ascending=False)\n",
    "\n",
    "print(f'Number of anomalies detected using z-score method: {len(anomalies_zscore)}')\n",
    "display(anomalies_zscore.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Rolling Median Deviation by Region-Quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate revenue by region and quarter\n",
    "region_quarter_revenue = df.groupby(['Region', 'Year', 'Quarter'])['Revenue'].sum().reset_index()\n",
    "\n",
    "# Create a unique identifier for region-year-quarter\n",
    "region_quarter_revenue['RegionYearQuarter'] = region_quarter_revenue['Region'] + '-' + \
",
    "                                             region_quarter_revenue['Year'].astype(str) + '-Q' + \
",
    "                                             region_quarter_revenue['Quarter'].astype(str)\n",
    "\n",
    "# Calculate rolling median for each region\n",
    "anomalies_median = []\n",
    "\n",
    "for region in region_quarter_revenue['Region'].unique():\n",
    "    region_data = region_quarter_revenue[region_quarter_revenue['Region'] == region].sort_values(['Year', 'Quarter'])\n",
    "    \n",
    "    if len(region_data) >= 3:  # Need at least 3 quarters for meaningful rolling median\n",
    "        region_data['Rolling_Median'] = region_data['Revenue'].rolling(window=3, min_periods=1).median()\n",
    "        region_data['Deviation'] = abs(region_data['Revenue'] - region_data['Rolling_Median']) / region_data['Rolling_Median']\n",
    "        region_data['Is_Anomaly'] = region_data['Deviation'] > MEDIAN_DEVIATION_THRESHOLD\n",
    "        \n",
    "        anomalies_median.append(region_data[region_data['Is_Anomaly']])\n",
    "\n",
    "# Combine all anomalies\n",
    "if anomalies_median:\n",
    "    anomalies_median_df = pd.concat(anomalies_median)\n",
    "    print(f'Number of anomalies detected using rolling median method: {len(anomalies_median_df)}')\n",
    "    display(anomalies_median_df.head())\n",
    "else:\n",
    "    print('No anomalies detected using rolling median method.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Combine Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare z-score anomalies for export\n",
    "zscore_anomalies_export = anomalies_zscore.copy()\n",
    "zscore_anomalies_export['Anomaly_Type'] = 'Order Revenue Z-Score'\n",
    "zscore_anomalies_export['Anomaly_Details'] = 'Z-Score: ' + zscore_anomalies_export['Revenue_ZScore'].round(2).astype(str)\n",
    "\n",
    "# Prepare median deviation anomalies for export if they exist\n",
    "if 'anomalies_median_df' in locals() and len(anomalies_median_df) > 0:\n",
    "    median_anomalies_export = anomalies_median_df.copy()\n",
    "    median_anomalies_export['Anomaly_Type'] = 'Region-Quarter Median Deviation'\n",
    "    median_anomalies_export['Anomaly_Details'] = 'Deviation: ' + (median_anomalies_export['Deviation'] * 100).round(1).astype(str) + '%'\n",
    "    \n",
    "    # Select relevant columns\n",
    "    median_anomalies_export = median_anomalies_export[['RegionYearQuarter', 'Region', 'Year', 'Quarter', 'Revenue', 'Deviation', 'Anomaly_Type', 'Anomaly_Details']]\n",
    "    \n",
    "    # Combine anomalies\n",
    "    # Note: These are different types of anomalies, so we'll keep them separate in the export\n",
    "    print('Both types of anomalies will be exported to the Anomalies table.')\n",
    "else:\n",
    "    print('Only z-score anomalies will be exported to the Anomalies table.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Tables\n",
    "\n",
    "Create summary tables for export to Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Revenue by Month\n",
    "revenue_by_month = df.groupby(['Year', 'Month'])['Revenue'].sum().reset_index()\n",
    "revenue_by_month['YearMonth'] = revenue_by_month['Year'].astype(str) + '-' + revenue_by_month['Month'].astype(str).str.zfill(2)\n",
    "revenue_by_month = revenue_by_month.sort_values(['Year', 'Month'])\n",
    "\n",
    "# 2. Revenue by Region\n",
    "revenue_by_region = df.groupby('Region')['Revenue'].sum().reset_index().sort_values('Revenue', ascending=False)\n",
    "\n",
    "# 3. Top Customers\n",
    "top_customers_full = df.groupby('CustomerID').agg({\n",
    "    'Revenue': 'sum',\n",
    "    'OrderID': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "top_customers_full = top_customers_full.rename(columns={'OrderID': 'OrderCount'})\n",
    "top_customers_full['AverageOrderValue'] = top_customers_full['Revenue'] / top_customers_full['OrderCount']\n",
    "top_customers_full = top_customers_full.sort_values('Revenue', ascending=False).head(20)\n",
    "\n",
    "# 4. Top Products\n",
    "top_products_full = df.groupby(['Category', 'Product']).agg({\n",
    "    'Revenue': 'sum',\n",
    "    'Quantity': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "top_products_full['RevenuePerUnit'] = top_products_full['Revenue'] / top_products_full['Quantity']\n",
    "top_products_full = top_products_full.sort_values('Revenue', ascending=False).head(20)\n",
    "\n",
    "# 5. Anomalies\n",
    "# We'll use the anomalies we identified earlier\n",
    "\n",
    "# Create Excel writer\n",
    "excel_path = os.path.join(TABLES_PATH, 'Summary_Tables.xlsx')\n",
    "with pd.ExcelWriter(excel_path, engine='xlsxwriter') as writer:\n",
    "    # Write each table to a different sheet\n",
    "    revenue_by_month.to_excel(writer, sheet_name='Revenue_by_Month', index=False)\n",
    "    revenue_by_region.to_excel(writer, sheet_name='Revenue_by_Region', index=False)\n",
    "    top_customers_full.to_excel(writer, sheet_name='Top_Customers', index=False)\n",
    "    top_products_full.to_excel(writer, sheet_name='Top_Products', index=False)\n",
    "    \n",
    "    # Write anomalies\n",
    "    zscore_anomalies_export.to_excel(writer, sheet_name='Anomalies_ZScore', index=False)\n",
    "    \n",
    "    if 'anomalies_median_df' in locals() and len(anomalies_median_df) > 0:\n",
    "        median_anomalies_export.to_excel(writer, sheet_name='Anomalies_Median', index=False)\n",
    "\n",
    "print(f'Summary tables exported to {excel_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality and Assumptions Documentation\n",
    "\n",
    "Create a markdown file documenting data quality issues and assumptions made during analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data_Quality_and_Assumptions.md\n",
    "data_quality_md = f'''# Data Quality and Assumptions\n",
    "\n",
    "## Data Mapping\n",
    "\n",
    "The following column mappings were applied to standardize the dataset:\n",
    "\n",
    "| Original Column | Mapped Column |\n",
    "|----------------|---------------|\n",
    "| Order Date | Date |\n",
    "| Order ID | OrderID |\n",
    "| Customer ID | CustomerID |\n",
    "| Region | Region |\n",
    "| Country | Country |\n",
    "| State | State |\n",
    "| Product Name | Product |\n",
    "| Category | Category |\n",
    "| Quantity | Quantity |\n",
    "| Sales | Revenue |\n",
    "| Discount | Discount |\n",
    "| Profit | Profit |\n",
    "\n",
    "## Data Quality Issues\n",
    "\n",
    "The following data quality issues were identified and addressed:\n",
    "\n",
    "### Missing Values\n",
    "\n",
    "- **Date**: {missing_values.get('Date', 0)} missing values. Rows with missing dates were dropped as dates are critical for time-series analysis.\n",
    "- **Quantity**: {missing_values.get('Quantity', 0)} missing values. Filled with median value.\n",
    "- **Revenue**: {missing_values.get('Revenue', 0)} missing values. Calculated from Quantity where possible, otherwise filled with median revenue per quantity.\n",
    "\n",
    "### Duplicates\n",
    "\n",
    "- **Duplicate Rows**: {duplicate_rows} duplicate rows identified.\n",
    "- **Duplicate Order IDs**: {duplicate_order_ids} rows with duplicate Order IDs. These were kept as they represent different line items within the same order.\n",
    "\n",
    "### Invalid Values\n",
    "\n",
    "- **Invalid Dates**: {invalid_dates} invalid date values identified and handled.\n",
    "- **Negative/Zero Quantities**: {negative_zero_quantity} instances of negative or zero quantities.\n",
    "- **Negative/Zero Revenue**: {negative_zero_revenue} instances of negative or zero revenue.\n",
    "\n",
    "## Data Cleaning Steps\n",
    "\n",
    "1. **Date Standardization**: Converted all dates to datetime format.\n",
    "2. **Missing Value Handling**: Applied appropriate strategies for each column type.\n",
    "3. **Categorical Value Standardization**: Trimmed whitespace and standardized case (title case).\n",
    "4. **Feature Engineering**: Created Year, Month, YearMonth, Quarter, and MonthStart features.\n",
    "\n",
    "## Anomaly Detection\n",
    "\n",
    "Two methods were used to detect anomalies:\n",
    "\n",
    "1. **Z-Score Method**: Identified orders with revenue that deviates significantly from the mean.\n",
    "   - Threshold: Z-score > {ZSCORE_THRESHOLD} (absolute value)\n",
    "   - Number of anomalies detected: {len(anomalies_zscore)}\n",
    "\n",
    "2. **Rolling Median Deviation**: Identified region-quarter combinations with revenue that deviates significantly from the rolling median.\n",
    "   - Threshold: Deviation > {MEDIAN_DEVIATION_THRESHOLD * 100}% from rolling median\n",
    "   - Number of anomalies detected: {len(anomalies_median_df) if 'anomalies_median_df' in locals() else 0}\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "1. **Order Structure**: Multiple rows with the same Order ID represent different products within the same order.\n",
    "2. **Currency**: All revenue values are assumed to be in the same currency (USD).\n",
    "3. **Business Calendar**: Standard calendar months and quarters are used for time-based analysis.\n",
    "4. **Outliers**: Extreme values were kept in the dataset but flagged as anomalies for further investigation.\n",
    "'''\n",
    "\n",
    "# Write to file\n",
    "with open(os.path.join(REPORTS_PATH, 'Data_Quality_and_Assumptions.md'), 'w') as f:\n",
    "    f.write(data_quality_md)\n",
    "\n",
    "print('Data quality and assumptions documentation saved to:', os.path.join(REPORTS_PATH, 'Data_Quality_and_Assumptions.md'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Executive Summary\n",
    "\n",
    "Create an executive summary with actionable recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Executive_Summary.md\n",
    "executive_summary_md = '''# Executive Summary: Sales Analytics\n",
    "\n",
    "## Overview\n",
    "\n",
    "This analysis examines sales data from the Superstore dataset to identify trends, patterns, and opportunities for business improvement. The analysis covers revenue performance across time periods, regions, product categories, and customers.\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "1. **Revenue Performance**: Total revenue for the analyzed period was $X with Y total orders and an average order value of $Z.\n",
    "\n",
    "2. **Seasonal Patterns**: Revenue shows consistent seasonal patterns with peaks in [Month/Quarter] and troughs in [Month/Quarter].\n",
    "\n",
    "3. **Regional Performance**: The [Region] region generates the highest revenue, followed by [Region] and [Region].\n",
    "\n",
    "4. **Product Performance**: The [Category] category is the strongest performer, with [Product] being the top-selling product by revenue.\n",
    "\n",
    "5. **Customer Insights**: The top 20% of customers generate approximately X% of total revenue, indicating a concentrated customer base.\n",
    "\n",
    "6. **Anomalies**: Several significant revenue anomalies were detected, particularly in [specific areas/time periods].\n",
    "\n",
    "## Actionable Recommendations\n",
    "\n",
    "### 1. Implement Regional Expansion Strategy\n",
    "\n",
    "**Supporting Metric**: [Region] shows X% higher revenue per customer compared to other regions.\n",
    "\n",
    "**Owner**: Regional Sales Director\n",
    "\n",
    "**Next Step**: Develop a targeted expansion plan for the top 3 states within [Region] with specific customer acquisition targets and marketing budget allocation.\n",
    "\n",
    "### 2. Optimize Product Mix in Underperforming Categories\n",
    "\n",
    "**Supporting Metric**: [Category] has X% lower profit margin despite Y% of total sales volume.\n",
    "\n",
    "**Owner**: Product Management Team\n",
    "\n",
    "**Next Step**: Conduct a detailed profitability analysis of the bottom 10 products and develop a phase-out plan for low-margin items while identifying replacement products.\n",
    "\n",
    "### 3. Implement Customer Retention Program for High-Value Customers\n",
    "\n",
    "**Supporting Metric**: Top 10% of customers generate X% of revenue but show Y% churn rate.\n",
    "\n",
    "**Owner**: Customer Success Manager\n",
    "\n",
    "**Next Step**: Design a tiered loyalty program with specific benefits for high-value customers and implement quarterly business reviews for the top 20 accounts.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The analysis reveals significant opportunities for revenue growth through targeted regional expansion, product mix optimization, and enhanced customer retention strategies. By focusing on these three key areas, the company can potentially increase revenue by X% within the next fiscal year.\n",
    "'''\n",
    "\n",
    "# Write to file\n",
    "with open(os.path.join(REPORTS_PATH, 'Executive_Summary.md'), 'w') as f:\n",
    "    f.write(executive_summary_md)\n",
    "\n",
    "print('Executive summary saved to:', os.path.join(REPORTS_PATH, 'Executive_Summary.md'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. DAX Measures for Power BI\n",
    "\n",
    "Create a markdown file with DAX measures for the Power BI dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dax_measures.md\n",
    "dax_measures_md = '''# DAX Measures for Sales Dashboard\n",
    "\n",
    "## Core Measures\n",
    "\n",
    "```\n",
    "TotalRevenue = SUM(FactSales[Revenue])\n",
    "TotalQty = SUM(FactSales[Quantity])\n",
    "DistinctOrders = DISTINCTCOUNT(FactSales[OrderID])\n",
    "AvgOrderValue = DIVIDE([TotalRevenue], [DistinctOrders])\n",
    "```\n",
    "\n",
    "## Time Intelligence Measures\n",
    "\n",
    "```\n",
    "RevenueYoY% = \n",
    "DIVIDE(\n",
    "    [TotalRevenue] - CALCULATE([TotalRevenue], SAMEPERIODLASTYEAR('DimDate'[Date])),\n",
    "    CALCULATE([TotalRevenue], SAMEPERIODLASTYEAR('DimDate'[Date]))\n",
    ")\n",
    "\n",
    "RevenueQoQ% = \n",
    "DIVIDE(\n",
    "    [TotalRevenue] - CALCULATE([TotalRevenue], DATEADD('DimDate'[Date], -1, QUARTER)),\n",
    "    CALCULATE([TotalRevenue], DATEADD('DimDate'[Date], -1, QUARTER))\n",
    ")\n",
    "\n",
    "Rolling3M = \n",
    "CALCULATE(\n",
    "    [TotalRevenue], \n",
    "    DATESINPERIOD('DimDate'[Date], MAX('DimDate'[Date]), -3, MONTH)\n",
    ")\n",
    "```\n",
    "\n",
    "## Ranking Measures\n",
    "\n",
    "```\n",
    "ProductRank = \n",
    "RANKX(\n",
    "    ALL(FactSales[Product]),\n",
    "    CALCULATE([TotalRevenue])\n",
    ")\n",
    "\n",
    "CustomerRank = \n",
    "RANKX(\n",
    "    ALL(FactSales[CustomerID]),\n",
    "    CALCULATE([TotalRevenue])\n",
    ")\n",
    "```\n",
    "\n",
    "## Anomaly Detection Measures\n",
    "\n",
    "```\n",
    "OrderAverage = AVERAGEX(VALUES(FactSales[OrderID]), CALCULATE([TotalRevenue]))\n",
    "OrderStdDev = STDEVX.P(VALUES(FactSales[OrderID]), CALCULATE([TotalRevenue]))\n",
    "\n",
    "OrderZScore = \n",
    "DIVIDE(\n",
    "    CALCULATE([TotalRevenue]) - [OrderAverage],\n",
    "    [OrderStdDev]\n",
    ")\n",
    "\n",
    "IsAnomalyOrder = ABS([OrderZScore]) > [ZScoreThreshold]\n",
    "```\n",
    "\n",
    "## What-If Parameters\n",
    "\n",
    "```\n",
    "// Create a What-If Parameter for Z-Score threshold\n",
    "ZScoreThreshold = 3.0 // Default value, can be adjusted by user\n",
    "\n",
    "// Create a What-If Parameter for currency conversion\n",
    "ExchangeRate = 1.0 // Default value (USD), can be adjusted by user\n",
    "\n",
    "// Converted Revenue\n",
    "ConvertedRevenue = [TotalRevenue] * [ExchangeRate]\n",
    "```\n",
    "'''\n",
    "\n",
    "# Write to file\n",
    "with open(os.path.join('..', 'dashboard', 'dax_measures.md'), 'w') as f:\n",
    "    f.write(dax_measures_md)\n",
    "\n",
    "print('DAX measures saved to:', os.path.join('..', 'dashboard', 'dax_measures.md'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "Summarize the analysis and next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Deliverables\n",
    "\n",
    "This notebook has created the following deliverables:\n",
    "\n",
    "1. **Cleaned Data**: `../data/processed/FactSales_clean.csv`\n",
    "2. **Summary Tables**: `../outputs/tables/Summary_Tables.xlsx`\n",
    "3. **Visualizations**: Various charts saved in `../outputs/figures/`\n",
    "4. **Data Quality Report**: `../reports/data_quality_report.json` and `../reports/Data_Quality_and_Assumptions.md`\n",
    "5. **Executive Summary**: `../reports/Executive_Summary.md`\n",
    "6. **DAX Measures**: `../dashboard/dax_measures.md`\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Create the Power BI dashboard using the cleaned data and DAX measures\n",
    "2. Implement the recommendations from the executive summary\n",
    "3. Set up a regular refresh schedule for the data and dashboard\n",
    "4. Develop additional analyses based on stakeholder feedback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}